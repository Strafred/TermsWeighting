{
 "cells": [
  {
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-10-26T09:05:15.412271Z",
     "start_time": "2024-10-26T08:58:49.908027Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pymystem3 import Mystem\n",
    "\n",
    "news_df = pd.DataFrame(data=None, columns=['filename', 'text'])\n",
    "news_df_index = 0\n",
    "m = Mystem(entire_input=False)\n",
    "\n",
    "with open('stop-words (Russian).txt', 'r') as file:\n",
    "    stop_words = file.read().split('\\n')\n",
    "\n",
    "def append_method_stats(filename, text):\n",
    "    global news_df_index\n",
    "    news_df.loc[news_df_index] = [filename, text]\n",
    "    news_df_index += 1\n",
    "\n",
    "for dirname, _, filenames in os.walk('texts'):\n",
    "    for filename in filenames:\n",
    "        filename = os.path.join(dirname, filename)\n",
    "        with open(filename, 'r') as file:\n",
    "            text = file.read()\n",
    "            text = m.lemmatize(text)\n",
    "            for word in text:\n",
    "                if word in stop_words:\n",
    "                    text.remove(word)\n",
    "            text = ' '.join(text)\n",
    "            append_method_stats(filename, text)\n",
    "            \n",
    "news_df.to_csv('news_df.csv', index=False)"
   ],
   "id": "initial_id",
   "outputs": [],
   "execution_count": 87
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-26T10:26:15.741033Z",
     "start_time": "2024-10-26T10:22:26.765866Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from sklearn.preprocessing import normalize\n",
    "from scipy import sparse\n",
    "\n",
    "\n",
    "def tfidf_vectorize(texts):\n",
    "    m = Mystem(entire_input=False)\n",
    "    words_set = set()\n",
    "    documents_count = len(texts)\n",
    "    words_dict = {}\n",
    "    texts_words_dict = {}\n",
    "    \n",
    "    for i in range(len(texts)):\n",
    "        if not isinstance(texts[i], (list, tuple, np.ndarray)):\n",
    "            all_words = m.lemmatize(texts[i])\n",
    "        else:\n",
    "            all_words = texts[i]\n",
    "        \n",
    "        for word in all_words:\n",
    "            words_set.add(word)\n",
    "            \n",
    "            if i not in texts_words_dict:\n",
    "                texts_words_dict[i] = {}\n",
    "            \n",
    "            if word not in texts_words_dict[i]:\n",
    "                texts_words_dict[i][word] = 1\n",
    "            else:\n",
    "                texts_words_dict[i][word] += 1\n",
    "        for word, _ in texts_words_dict[i].items():\n",
    "            if word not in words_dict:\n",
    "                words_dict[word] = 1\n",
    "            else:\n",
    "                words_dict[word] += 1\n",
    "\n",
    "    num_words = len(words_set)\n",
    "            \n",
    "    vectors = np.zeros((len(texts), num_words))\n",
    "    for i in range(len(texts)):\n",
    "        vector = np.zeros(num_words)\n",
    "        for j, word in enumerate(words_set):\n",
    "            if word in texts_words_dict[i]:\n",
    "                tf = texts_words_dict[i][word] / len(texts[i])\n",
    "                idf = np.log((1 + documents_count) / (1 + words_dict[word])) + 1\n",
    "                vector[j] = tf * idf\n",
    "        \n",
    "        vector_norm = np.sqrt(np.sum(vector ** 2))\n",
    "        for k in range(num_words):\n",
    "            vector[k] /= vector_norm\n",
    "        vectors[i] = vector\n",
    "        \n",
    "    return sparse.csr_matrix(vectors), list(words_set)\n",
    "\n",
    "def freq_vectorize(texts):\n",
    "    m = Mystem(entire_input=False)\n",
    "    words_set = set()\n",
    "    texts_words_dict = {}\n",
    "    \n",
    "    for i in range(len(texts)):\n",
    "        if not isinstance(texts[i], (list, tuple, np.ndarray)):\n",
    "            all_words = m.lemmatize(texts[i])\n",
    "        else:\n",
    "            all_words = texts[i]\n",
    "            \n",
    "        for word in all_words:\n",
    "            words_set.add(word)\n",
    "            \n",
    "            if i not in texts_words_dict:\n",
    "                texts_words_dict[i] = {}\n",
    "            \n",
    "            if word not in texts_words_dict[i]:\n",
    "                texts_words_dict[i][word] = 1\n",
    "            else:\n",
    "                texts_words_dict[i][word] += 1\n",
    "\n",
    "    num_words = len(words_set)\n",
    "            \n",
    "    vectors = np.zeros((len(texts), num_words))\n",
    "    for i in range(len(texts)):\n",
    "        vector = np.zeros(num_words)\n",
    "        for j, word in enumerate(words_set):\n",
    "            if word in texts_words_dict[i]:\n",
    "                vector[j] = texts_words_dict[i][word]\n",
    "        vectors[i] = vector\n",
    "        \n",
    "    return sparse.csr_matrix(vectors), list(words_set)\n",
    "\n",
    "def normfreq_vectorize(texts):\n",
    "    m = Mystem(entire_input=False)\n",
    "    words_set = set()\n",
    "    texts_words_dict = {}\n",
    "    \n",
    "    for i in range(len(texts)):\n",
    "        if not isinstance(texts[i], (list, tuple, np.ndarray)):\n",
    "            all_words = m.lemmatize(texts[i])\n",
    "        else:\n",
    "            all_words = texts[i]\n",
    "            \n",
    "        for word in all_words:\n",
    "            words_set.add(word)\n",
    "            \n",
    "            if i not in texts_words_dict:\n",
    "                texts_words_dict[i] = {}\n",
    "            \n",
    "            if word not in texts_words_dict[i]:\n",
    "                texts_words_dict[i][word] = 1\n",
    "            else:\n",
    "                texts_words_dict[i][word] += 1\n",
    "\n",
    "    num_words = len(words_set)\n",
    "            \n",
    "    vectors = np.zeros((len(texts), num_words))\n",
    "    for i in range(len(texts)):\n",
    "        vector = np.zeros(num_words)\n",
    "        for j, word in enumerate(words_set):\n",
    "            if word in texts_words_dict[i]:\n",
    "                vector[j] = texts_words_dict[i][word]\n",
    "        vectors[i] = vector\n",
    "        \n",
    "    return normalize(sparse.csr_matrix(vectors), norm=\"l1\", axis=1), list(words_set)\n",
    "\n",
    "def vectorize_texts(df, method='tfidf'):\n",
    "    if method == 'tfidf':\n",
    "        vectors, terms = tfidf_vectorize(df['text'])\n",
    "        return vectors, terms\n",
    "    elif method == 'freq':\n",
    "        vectors, terms = freq_vectorize(df['text'])\n",
    "        return vectors, terms\n",
    "    elif method == 'normfreq':\n",
    "        vectors, terms = normfreq_vectorize(df['text'])\n",
    "        return vectors, terms\n",
    "    \n",
    "tfidf_vectors, tfidf_terms = vectorize_texts(news_df, method='tfidf')\n",
    "freq_vectors, freq_terms = vectorize_texts(news_df, method='freq')\n",
    "normfreq_vectors, normfreq_terms = vectorize_texts(news_df, method='normfreq')\n",
    "\n",
    "sparse.save_npz('vectors/tfidf_vectors.npz', tfidf_vectors)\n",
    "np.save('vectors/tfidf_terms.npy', tfidf_terms)\n",
    "sparse.save_npz('vectors/freq_vectors.npz', freq_vectors)\n",
    "np.save('vectors/freq_terms.npy', freq_terms)\n",
    "# print(freq_vectors[:10])\n",
    "sparse.save_npz('vectors/normfreq_vectors.npz', normfreq_vectors)\n",
    "np.save('vectors/normfreq_terms.npy', normfreq_terms)"
   ],
   "id": "80869233bd80d7f3",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 66)\t2.0\n",
      "  (0, 83)\t1.0\n",
      "  (0, 85)\t3.0\n",
      "  (0, 92)\t1.0\n",
      "  (0, 142)\t1.0\n",
      "  (0, 256)\t2.0\n",
      "  (0, 266)\t1.0\n",
      "  (0, 308)\t1.0\n",
      "  (0, 310)\t1.0\n",
      "  (0, 498)\t1.0\n",
      "  (0, 510)\t1.0\n",
      "  (0, 584)\t1.0\n",
      "  (0, 664)\t1.0\n",
      "  (0, 758)\t1.0\n",
      "  (0, 824)\t1.0\n",
      "  (0, 833)\t1.0\n",
      "  (0, 1011)\t1.0\n",
      "  (0, 1060)\t1.0\n",
      "  (0, 1075)\t1.0\n",
      "  (0, 1129)\t2.0\n",
      "  (0, 1133)\t1.0\n",
      "  (0, 1153)\t1.0\n",
      "  (0, 1165)\t1.0\n",
      "  (0, 1182)\t1.0\n",
      "  (0, 1190)\t3.0\n",
      "  :\t:\n",
      "  (9, 1273)\t3.0\n",
      "  (9, 1277)\t1.0\n",
      "  (9, 1294)\t2.0\n",
      "  (9, 1303)\t1.0\n",
      "  (9, 1412)\t1.0\n",
      "  (9, 1445)\t1.0\n",
      "  (9, 1490)\t1.0\n",
      "  (9, 1495)\t1.0\n",
      "  (9, 1500)\t1.0\n",
      "  (9, 1515)\t1.0\n",
      "  (9, 1533)\t1.0\n",
      "  (9, 1590)\t2.0\n",
      "  (9, 1598)\t2.0\n",
      "  (9, 1650)\t2.0\n",
      "  (9, 1673)\t1.0\n",
      "  (9, 1805)\t1.0\n",
      "  (9, 1919)\t1.0\n",
      "  (9, 1921)\t1.0\n",
      "  (9, 1981)\t3.0\n",
      "  (9, 2019)\t1.0\n",
      "  (9, 2062)\t1.0\n",
      "  (9, 2083)\t2.0\n",
      "  (9, 2134)\t2.0\n",
      "  (9, 2188)\t1.0\n",
      "  (9, 2265)\t1.0\n"
     ]
    }
   ],
   "execution_count": 132
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-26T10:06:24.849023Z",
     "start_time": "2024-10-26T10:06:24.839995Z"
    }
   },
   "cell_type": "code",
   "source": "tfidf_vectors.shape",
   "id": "2592fe7523ad2eae",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(74, 2325)"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 123
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-26T10:20:50.234192Z",
     "start_time": "2024-10-26T10:20:50.228488Z"
    }
   },
   "cell_type": "code",
   "source": "print(sparse.csr_matrix([1, 2, 3, 4, 5]))",
   "id": "1afeb4079bafb3b2",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 0)\t1\n",
      "  (0, 1)\t2\n",
      "  (0, 2)\t3\n",
      "  (0, 3)\t4\n",
      "  (0, 4)\t5\n"
     ]
    }
   ],
   "execution_count": 131
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-26T10:06:24.936697Z",
     "start_time": "2024-10-26T10:06:24.929777Z"
    }
   },
   "cell_type": "code",
   "source": "freq_vectors.shape",
   "id": "398c51ee6c93f7c6",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(74, 2325)"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 124
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-26T10:06:25.099032Z",
     "start_time": "2024-10-26T10:06:25.093396Z"
    }
   },
   "cell_type": "code",
   "source": "normfreq_vectors.shape",
   "id": "7246e7904b1e9382",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(74, 2325)"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 125
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-26T10:06:25.189561Z",
     "start_time": "2024-10-26T10:06:25.183147Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import time\n",
    "\n",
    "\n",
    "def add_document_to_collection(filepath):\n",
    "    news_df = pd.read_csv('news_df.csv')\n",
    "    news_df_index = news_df.shape[0]\n",
    "\n",
    "    with open('stop-words (Russian).txt', 'r') as file:\n",
    "        stop_words = file.read().split('\\n')\n",
    "\n",
    "    with open(filepath, 'r') as file:\n",
    "        text = file.read()\n",
    "        text = m.lemmatize(text)\n",
    "        for word in text:\n",
    "            if word in stop_words:\n",
    "                text.remove(word)\n",
    "        text = ' '.join(text)\n",
    "        news_df.loc[news_df_index] = [filename, text]\n",
    "\n",
    "    news_df.to_csv('news_df.csv', index=False)\n",
    "    print(news_df.shape)\n",
    "\n",
    "    start_time = time.time()\n",
    "    tfidf_vectors, tfidf_terms = vectorize_texts(news_df, method='tfidf')\n",
    "    print(tfidf_vectors.shape)\n",
    "    print(f\"TF-IDF model vectorization time: {time.time() - start_time}\")\n",
    "\n",
    "    start_time = time.time()\n",
    "    freq_vectors, freq_terms = vectorize_texts(news_df, method='freq')\n",
    "    print(freq_vectors.shape)\n",
    "    print(f\"Frequency model vectorization time: {time.time() - start_time}\")\n",
    "\n",
    "    start_time = time.time()\n",
    "    normfreq_vectors, normfreq_terms = vectorize_texts(news_df, method='normfreq')\n",
    "    print(normfreq_vectors.shape)\n",
    "    print(f\"Normalized frequency model vectorization time: {time.time() - start_time}\")\n",
    "\n",
    "    sparse.save_npz('vectors/tfidf_vectors.npz', tfidf_vectors)\n",
    "    np.save('vectors/tfidf_terms.npy', tfidf_terms)\n",
    "    sparse.save_npz('vectors/freq_vectors.npz', freq_vectors)\n",
    "    np.save('vectors/freq_terms.npy', freq_terms)\n",
    "    sparse.save_npz('vectors/normfreq_vectors.npz', normfreq_vectors)\n",
    "    np.save('vectors/normfreq_terms.npy', normfreq_terms)\n",
    "\n",
    "add_document_to_collection('Финляндия.txt')"
   ],
   "id": "1685d2ef7c5e02a0",
   "outputs": [],
   "execution_count": 126
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-26T10:06:25.336271Z",
     "start_time": "2024-10-26T10:06:25.247703Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def choose_top_10(array):\n",
    "    top_10_indexes = []\n",
    "    top_10_values = []\n",
    "    for i in range(10):\n",
    "        max_index = np.argmax(array)\n",
    "        top_10_indexes.append(max_index)\n",
    "        top_10_values.append(array[max_index])\n",
    "        array[max_index] = 0\n",
    "    return top_10_indexes, top_10_values\n",
    "\n",
    "def get_top_terms(vectors, terms):\n",
    "    news_df = pd.read_csv('news_df.csv')\n",
    "    text_names = news_df['filename']\n",
    "    df = pd.DataFrame(data=None, columns=['text', 'top_terms'])\n",
    "    df_index = 0\n",
    "    \n",
    "    rows, _ = vectors.shape\n",
    "    vectors = vectors.toarray()\n",
    "    for i in range(rows):\n",
    "        top_10_indexes, top_10_values = choose_top_10(vectors[i, :])\n",
    "        top_terms_json = {}\n",
    "        for index, value in zip(top_10_indexes, top_10_values):\n",
    "            top_terms_json[terms[index]] = value\n",
    "        df.loc[df_index] = [text_names[i], top_terms_json]\n",
    "        df_index += 1\n",
    "    return df\n",
    "        \n",
    "top_tf_idf_terms = get_top_terms(tfidf_vectors, tfidf_terms)\n",
    "top_tf_idf_terms.to_csv('top_tf_idf_terms.csv', index=False)"
   ],
   "id": "ba5dbb1368db10d1",
   "outputs": [],
   "execution_count": 127
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-26T10:06:25.416383Z",
     "start_time": "2024-10-26T10:06:25.340300Z"
    }
   },
   "cell_type": "code",
   "source": [
    "top_freq_terms = get_top_terms(freq_vectors, freq_terms)\n",
    "top_freq_terms.to_csv('top_freq_terms.csv', index=False)"
   ],
   "id": "7cf123fdb6c432ed",
   "outputs": [],
   "execution_count": 128
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-26T10:06:25.524449Z",
     "start_time": "2024-10-26T10:06:25.445158Z"
    }
   },
   "cell_type": "code",
   "source": [
    "top_normfreq_terms = get_top_terms(normfreq_vectors, normfreq_terms)\n",
    "top_normfreq_terms.to_csv('top_normfreq_terms.csv', index=False)"
   ],
   "id": "6da207c2744137a4",
   "outputs": [],
   "execution_count": 129
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
